{"cells":[{"cell_type":"markdown","metadata":{"id":"AfhobhIVmRNV"},"source":["## Download Promela"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1500,"status":"ok","timestamp":1767102531006,"user":{"displayName":"Minh Nguy·ªÖn","userId":"04212426870901819046"},"user_tz":-420},"id":"L1Gc4E7ClS4j","outputId":"5491e06e-e0e9-4592-f744-005527ec572d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Successfully logged in to Hugging Face using Colab secrets!\n"]}],"source":["from huggingface_hub import login\n","from google.colab import userdata\n","import pandas as pd\n","\n","hf_token = userdata.get('sec') # Get the token from Colab secrets\n","if hf_token:\n","    login(token=hf_token)\n","    print(\"Successfully logged in to Hugging Face using Colab secrets!\")\n","else:\n","    print(\"HF_TOKEN not found in Colab secrets.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12818,"status":"ok","timestamp":1767102543833,"user":{"displayName":"Minh Nguy·ªÖn","userId":"04212426870901819046"},"user_tz":-420},"id":"2Wn56vsGugeF","outputId":"323eab63-5a9f-477d-cb5b-b86d29d60369"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: boto3 in /usr/local/lib/python3.12/dist-packages (1.42.18)\n","Requirement already satisfied: botocore<1.43.0,>=1.42.18 in /usr/local/lib/python3.12/dist-packages (from boto3) (1.42.18)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3) (1.0.1)\n","Requirement already satisfied: s3transfer<0.17.0,>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from boto3) (0.16.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.43.0,>=1.42.18->boto3) (2.9.0.post0)\n","Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.43.0,>=1.42.18->boto3) (2.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.43.0,>=1.42.18->boto3) (1.17.0)\n"]}],"source":["!pip install boto3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UObIcdqebQAK"},"outputs":[],"source":["\n","# root = \"/content/drive/MyDrive/ThaÃ£c siÃÉ/IT6481 - KieÃÇÃâm chuÃõÃÅng vaÃÄ thaÃÇÃâm ƒëiÃ£nh phaÃÇÃÄn meÃÇÃÄm/TieÃÇÃâu luaÃ£ÃÇn Sinh Promela\"\n","# # Read the parquet file into a pandas DataFrame\n","# df = pd.read_parquet(f'{root}/Data/train-00000-of-00001.parquet')\n","\n","# # Display the first few rows\n","# print(df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZdYHuQEGIce"},"outputs":[],"source":["import os\n","import boto3\n","from smart_open import open\n","from datasets import load_dataset\n","from botocore import UNSIGNED\n","from botocore.client import Config\n","\n","# C·∫•u h√¨nh S3\n","s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n","\n","def download_contents(blob_id, src_encoding):\n","    s3_url = f\"s3://softwareheritage/content/{blob_id}\"\n","    try:\n","        with open(s3_url, \"rb\", compression=\".gz\", transport_params={\"client\": s3}) as fin:\n","            content = fin.read().decode(src_encoding)\n","        return content\n","    except Exception as e:\n","        return f\"Error downloading {blob_id}: {str(e)}\"\n","\n","# df = pd.read_parquet('train-00000-of-00001.parquet')\n","\n","# for data in df:\n","# # content = download_contents(row[\"blob_id\"], row[\"src_encoding\"])\n","# print(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijEwqQcYG4yh"},"outputs":[],"source":["lstBloID = list(df[\"blob_id\"])\n","lstSrcID = list(df[\"src_encoding\"])\n","\n","\n","# output_file = f\"{root}/Data/promela_dataset.jsonl\"\n","\n","# print(f\"üíæ ƒêang l∆∞u v√†o {output_file}...\")\n","\n","# with open(output_file, 'w', encoding='utf-8') as f:\n","#     for i in range(len(lstBloID)):\n","#       content = download_contents(lstBloID[i], lstSrcID[i])\n","#       f.write(json.dumps(content, ensure_ascii=False) + '\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xgCsVaT-xORJ"},"outputs":[],"source":["# for i in range(len(lstBloID)):\n","#       if ( i == 5 ):\n","#         break;\n","#       print(\"------\"*20)\n","#       content = download_contents(lstBloID[i], lstSrcID[i])\n","#       print(content)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"elapsed":21,"status":"error","timestamp":1767102525837,"user":{"displayName":"Minh Nguy·ªÖn","userId":"04212426870901819046"},"user_tz":-420},"id":"N4OOUy-7xQvp","outputId":"f0cf8038-5da3-4020-c42e-4d8f7db9d4a5"},"outputs":[{"ename":"NameError","evalue":"name 'content' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-433833203.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"]}],"source":["print(content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"40HsT5Udxktk"},"outputs":[],"source":["# import os\n","# import json\n","# import boto3\n","# import pandas as pd\n","# from smart_open import open\n","# from botocore import UNSIGNED\n","# from botocore.client import Config\n","# from tqdm import tqdm  # Th∆∞ vi·ªán t·∫°o thanh ti·∫øn tr√¨nh\n","\n","# # 1. C·∫•u h√¨nh\n","# OUTPUT_DIR = \"promela_source_files\"  # Th∆∞ m·ª•c ch·ª©a c√°c file .pml ri√™ng l·∫ª\n","# JSONL_FILE = \"promela_dataset.jsonl\" # File t·ªïng h·ª£p d·∫°ng jsonl\n","# S3_BUCKET = \"softwareheritage\"\n","\n","# # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥\n","# os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","# # C·∫•u h√¨nh S3 Client (Public access)\n","# s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n","\n","# # Gi·∫£ l·∫≠p bi·∫øn df n·∫øu b·∫°n ch∆∞a ch·∫°y ƒëo·∫°n tr∆∞·ªõc (B·∫°n c√≥ th·ªÉ b·ªè qua d√≤ng n√†y n·∫øu df ƒë√£ c√≥)\n","# # df = pd.DataFrame({'blob_id': [...], 'src_encoding': [...]})\n","\n","# def download_contents(blob_id, src_encoding):\n","#     \"\"\"\n","#     H√†m t·∫£i n·ªôi dung t·ª´ S3 Software Heritage\n","#     \"\"\"\n","#     # L∆∞u √Ω: C·∫•u tr√∫c key c·ªßa SWH tr√™n S3 th∆∞·ªùng l√† content/<sha1_git>\n","#     s3_url = f\"s3://{S3_BUCKET}/content/{blob_id}\"\n","\n","#     try:\n","#         # X·ª≠ l√Ω encoding: n·∫øu kh√¥ng c√≥ encoding ho·∫∑c l√† 'None', m·∫∑c ƒë·ªãnh l√† utf-8\n","#         encoding = src_encoding if src_encoding and src_encoding != 'None' else 'utf-8'\n","\n","#         # smart_open h·ªó tr·ª£ ƒë·ªçc tr·ª±c ti·∫øp t·ª´ S3 url\n","#         with open(s3_url, \"rb\", compression=\".gz\", transport_params={\"client\": s3}) as fin:\n","#             # C·ªë g·∫Øng decode, n·∫øu l·ªói th√¨ d√πng 'replace' ƒë·ªÉ tr√°nh crash ch∆∞∆°ng tr√¨nh\n","#             content = fin.read().decode(encoding, errors='replace')\n","#         return content\n","#     except Exception as e:\n","#         print(f\"Error downloading {blob_id}: {str(e)}\")\n","#         return None\n","\n","# def save_data(df):\n","#     lstBloID = list(df[\"blob_id\"])\n","#     lstSrcID = list(df[\"src_encoding\"])\n","\n","#     # M·ªü file JSONL ·ªü ch·∫ø ƒë·ªô 'a' (append) ƒë·ªÉ ghi t·ª´ng d√≤ng, tr√°nh t·ªën RAM gi·ªØ list qu√° l·ªõn\n","#     with open(JSONL_FILE, 'w', encoding='utf-8') as f_json:\n","\n","#         # S·ª≠ d·ª•ng tqdm ƒë·ªÉ hi·ªán thanh ti·∫øn tr√¨nh\n","#         for blob_id, src_encoding in tqdm(zip(lstBloID, lstSrcID), total=len(lstBloID), desc=\"Downloading Promela\"):\n","\n","#             content = download_contents(blob_id, src_encoding)\n","\n","#             if content:\n","#                 # --- D·∫°ng 1: L∆∞u th√†nh t·ª´ng file .pml ---\n","#                 file_path = os.path.join(OUTPUT_DIR, f\"{blob_id}.pml\")\n","#                 try:\n","#                     with open(file_path, \"w\", encoding=\"utf-8\") as f_pml:\n","#                         f_pml.write(content)\n","#                 except Exception as e:\n","#                     print(f\"Error writing file {file_path}: {e}\")\n","\n","#                 # --- D·∫°ng 2: Ghi v√†o file JSONL ---\n","#                 record = {\n","#                     \"blob_id\": blob_id,\n","#                     \"encoding\": src_encoding,\n","#                     \"content\": content\n","#                 }\n","#                 # dumps bi·∫øn dict th√†nh string json v√† ghi v√†o file, th√™m xu·ªëng d√≤ng\n","#                 f_json.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n","\n","# # --- Th·ª±c thi ---\n","# # ƒê·∫£m b·∫£o bi·∫øn df c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c load t·ª´ tr∆∞·ªõc ƒë√≥\n","# if 'df' in locals():\n","#     print(\"B·∫Øt ƒë·∫ßu t·∫£i m√£ ngu·ªìn Promela...\")\n","#     save_data(df)\n","#     print(f\"\\nHo√†n t·∫•t! \\n- File l·∫ª t·∫°i: {OUTPUT_DIR}/ \\n- File t·ªïng h·ª£p: {JSONL_FILE}\")\n","# else:\n","#     print(\"Vui l√≤ng ƒë·ªãnh nghƒ©a DataFrame 'df' ch·ª©a c·ªôt 'blob_id' v√† 'src_encoding' tr∆∞·ªõc khi ch·∫°y.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qq6o-tcCMplF"},"outputs":[],"source":["# !zip -r /content/d√¢t.zip /content/promela_source_files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Cm4sdq_oTkz"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"_wg5B_nloWj5"},"source":["## Generate Data Promt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KiEIa3V0o-ai"},"outputs":[],"source":["import os\n","import json\n","import time\n","import glob\n","import logging\n","from typing import List, Dict, Optional\n","import google.generativeai as genai\n","from google.api_core import exceptions\n","from google.genai import types\n","from tenacity import (\n","    retry,\n","    stop_after_attempt,\n","    wait_exponential,\n","    retry_if_exception_type,\n","    before_sleep_log\n",")\n","from tqdm import tqdm\n","from dotenv import load_dotenv\n","\n","# --- C·∫§U H√åNH ---\n","load_dotenv() # Load API Key t·ª´ file.env\n","GOOGLE_API_KEY = os.getenv(\"AIzaSyD0IJNSgAubQ-YfYauzgWJEGqSyYv8NzyU\")\n","\n","# C·∫•u h√¨nh Gemini\n","genai.configure(api_key=\"AIzaSyD0IJNSgAubQ-YfYauzgWJEGqSyYv8NzyU\")\n","\n","# Ch·ªçn model: 'gemini-1.5-flash' (nhanh, r·∫ª) ho·∫∑c 'gemini-1.5-pro' (logic t·ªët h∆°n)\n","MODEL_NAME = \"gemini-2.5-flash\"\n","\n","# C·∫•u h√¨nh Generation\n","GENERATION_CONFIG = {\n","    \"temperature\": 0.2, # Th·∫•p ƒë·ªÉ gi·∫£m ·∫£o gi√°c, gi·ªØ t√≠nh ch√≠nh x√°c k·ªπ thu·∫≠t\n","    \"top_p\": 0.95,\n","    \"top_k\": 64,\n","    \"max_output_tokens\": 8192,\n","    \"response_mime_type\": \"application/json\", # B·∫Øt bu·ªôc Gemini tr·∫£ v·ªÅ JSON,\n","}\n","\n","# Prompt h·ªá th·ªëng chuy√™n gia\n","SYSTEM_INSTRUCTION = \"\"\"\n","You are an expert in Formal Methods, specifically the SPIN model checker and Promela language.\n","Your task is to analyze Promela source code and reverse-engineer the software requirements.\n","Output must be a JSON object with these fields:\n","- \"summary\": A concise summary of what the model does.\n","- \"processes\": A list of processes defined in the code.\n","- \"properties\": A list of LTL formulas or assertions checked.\n","- \"instruction\": A clear, natural language prompt that could be used to generate this code.\n","\"\"\"\n","\n","# Setup Logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","load_dotenv()\n","class GeminiGenerator:\n","    def __init__(self):\n","        self.model = genai.GenerativeModel(\n","            model_name=MODEL_NAME,\n","            system_instruction=SYSTEM_INSTRUCTION,\n","            generation_config=GENERATION_CONFIG\n","        )\n","\n","    @retry(\n","        retry=retry_if_exception_type((\n","            exceptions.ResourceExhausted, # L·ªói 429 Rate Limit\n","            exceptions.ServiceUnavailable, # L·ªói 503\n","            exceptions.GoogleAPIError\n","        )),\n","        wait=wait_exponential(multiplier=1, min=60, max=120), # Ch·ªù 4s, 8s, 16s...\n","        stop=stop_after_attempt(5),\n","        before_sleep=before_sleep_log(logger, logging.WARNING)\n","    )\n","    def generate_description(self, code_content: str) -> Optional:\n","        try:\n","            # T·∫°o prompt\n","            prompt = f\"Analyze this Promela code:\\n\\n```promela\\n{code_content}\\n```\"\n","\n","            # G·ªçi API\n","            response = self.model.generate_content(prompt)\n","\n","            # Parse JSON t·ª´ response text\n","            return json.loads(response.text)\n","\n","        except Exception as e:\n","            logger.error(f\"Error generating content: {e}\")\n","            raise e # N√©m l·ªói ƒë·ªÉ trigger retry\n","\n","    def testGenerate(self,content:str):\n","      return self.model.generate_content(content)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JRu3Isfdm7gR"},"outputs":[],"source":["# !free -h"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225},"executionInfo":{"elapsed":3077,"status":"ok","timestamp":1767266709536,"user":{"displayName":"Minh Nguy·ªÖn","userId":"04212426870901819046"},"user_tz":-420},"id":"TzMwjIOgpwGI","outputId":"42678324-0ec2-49fa-bcdb-53286eca4497"},"outputs":[{"name":"stderr","output_type":"stream","text":["<>:2: SyntaxWarning: invalid escape sequence '\\{'\n","<>:2: SyntaxWarning: invalid escape sequence '\\{'\n","/tmp/ipython-input-4063137960.py:2: SyntaxWarning: invalid escape sequence '\\{'\n","  generator1.generate_description(\"active proctype P() \\{ int x = 0; x++; \\}\")\n"]},{"data":{"text/plain":["{'summary': \"This model defines a single active process 'P' that initializes an integer variable 'x' to 0 and then increments it once. The process executes these steps and then terminates.\",\n"," 'processes': ['P'],\n"," 'properties': [],\n"," 'instruction': \"Create a Promela model with a single active process named 'P'. Inside process 'P', declare an integer variable 'x', initialize it to 0, and then increment 'x' by one.\"}"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":[" generator1 = GeminiGenerator()\n"," generator1.generate_description(\"active proctype P() \\{ int x = 0; x++; \\}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8cKk35HphcmE"},"outputs":[],"source":["# for m in genai.list_models():\n","#     if 'generateContent' in m.supported_generation_methods:\n","#         print(f\"- {m.name}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"bp8okb09zJnV","outputId":"2b9c13eb-35f4-4c57-aa79-20184c9cc7a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω t·ª´ file: clean_data.jsonl\n"]},{"name":"stderr","output_type":"stream","text":["Generating: 0it [00:00, ?it/s]WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 556.36ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 21.532579833s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 21.532579833s..\n","WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 755.04ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 20.789014845s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 20.789014845s..\n","WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 730.28ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 20.037391737s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 20.037391737s..\n","WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 655.26ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 19.384161034s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 19.384161034s..\n","WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 831.04ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 18.57331246s.\n","ERROR:__main__:L·ªói d√≤ng: RetryError[<Future at 0x7b3ee71a2a80 state=finished raised TooManyRequests>]\n","Generating: 1it [04:03, 243.56s/it]WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 554.87ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 18.010231023s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 18.010231023s..\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 17.151829868s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 17.151829868s..\n","WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 805.70ms\n","Generating: 2it [06:23, 182.68s/it]WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 579.39ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 57.908163156s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 57.908163156s..\n","WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 680.33ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 57.212942231s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 57.212942231s..\n","WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 756.07ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 56.466398257s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 56.466398257s..\n","WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 805.94ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 55.662312748s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 55.662312748s..\n","ERROR:__main__:Error generating content: Expecting ',' delimiter: line 4 column 9 (char 728)\n","ERROR:__main__:L·ªói d√≤ng: Expecting ',' delimiter: line 4 column 9 (char 728)\n","Generating: 3it [10:40, 216.54s/it]WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 605.68ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 40.993355359s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 40.993355359s..\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 40.300153831s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 40.300153831s..\n","WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 734.64ms\n","WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 680.25ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 39.594211407s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 39.594211407s..\n","WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 755.49ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 38.825242156s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 38.825242156s..\n","WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 806.64ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 38.025815702s.\n","ERROR:__main__:L·ªói d√≤ng: RetryError[<Future at 0x7b3ee710a000 state=finished raised TooManyRequests>]\n","Generating: 5it [14:44, 162.92s/it]WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 528.90ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 37.51292564s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 37.51292564s..\n","WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 780.46ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 36.717315879s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 36.717315879s..\n","WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 680.14ms\n","ERROR:__main__:Error generating content: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 36.023628454s.\n","WARNING:__main__:Retrying __main__.GeminiGenerator.generate_description in 60.0 seconds as it raised TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n","* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n","Please retry in 36.023628454s..\n","Generating: 5it [17:22, 208.51s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-205870480.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-205870480.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;31m# G·ªçi Gemini\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoSleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_for_next_attempt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdo\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/nap.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmocked\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \"\"\"\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["def main():\n","    # C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\n","    input_jsonl = \"clean_data.jsonl\"  # File ch·ª©a code ƒë·∫ßu v√†o\n","    output_file = \"gemini_finetuning_data.jsonl\" # File k·∫øt qu·∫£\n","\n","    # Kh·ªüi t·∫°o Generator\n","    generator = GeminiGenerator()\n","\n","    print(f\"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω t·ª´ file: {input_jsonl}\")\n","\n","    # M·ªü 2 file: 1 ƒë·ªÉ ƒë·ªçc, 1 ƒë·ªÉ ghi n·ªëi ti·∫øp (append)\n","    with open(input_jsonl, 'r', encoding='utf-8') as f_in, \\\n","         open(output_file, 'a', encoding='utf-8') as f_out:\n","\n","        # D√πng tqdm ƒë·ªÉ hi·ªán thanh ti·∫øn tr√¨nh (n·∫øu bi·∫øt t·ªïng s·ªë d√≤ng th√¨ th√™m total=...)\n","        for line in tqdm(f_in, desc=\"Generating\"):\n","            try:\n","                if not line.strip(): continue\n","                record = json.loads(line)\n","\n","                # L·∫•y code (gi·∫£ s·ª≠ c·ªôt t√™n l√† 'content')\n","                code = record.get('content', '')\n","\n","                # B·ªçc ƒëi·ªÅu ki·ªán l·ªçc\n","                if len(code) < 50 or len(code) > 30000: continue\n","\n","                # G·ªçi Gemini\n","                analysis = generator.generate_description(code)\n","\n","                if analysis:\n","                    # T·∫°o format ShareGPT\n","                    train_sample = {\n","                        \"conversations\": [\n","                            {\"from\": \"human\", \"value\": analysis['instruction']},\n","                            {\"from\": \"gpt\", \"value\": code}\n","                        ],\n","                        # Gi·ªØ l·∫°i metadata g·ªëc ƒë·ªÉ d·ªÖ tra c·ª©u (v√≠ d·ª• ID file)\n","                        \"metadata\": {\n","                            \"original_id\": record.get('blob_id'),\n","                            \"summary\": analysis['summary']\n","                        }\n","                    }\n","\n","                    # Ghi ngay l·∫≠p t·ª©c\n","                    f_out.write(json.dumps(train_sample, ensure_ascii=False) + '\\n')\n","                    f_out.flush()\n","\n","                    # Rate limit sleep\n","                    time.sleep(10)\n","\n","            except Exception as e:\n","                # Log l·ªói nh∆∞ng kh√¥ng d·ª´ng ch∆∞∆°ng tr√¨nh\n","                logger.error(f\"L·ªói d√≤ng: {str(e)[:100]}\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tdzE4XlSjce"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8s49YqlyCo91"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mefS6QWMCqZ0"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"hVB0NzuQCrbz"},"source":["## GPT oss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":55654,"status":"ok","timestamp":1767262379655,"user":{"displayName":"Minh Nguy·ªÖn","userId":"04212426870901819046"},"user_tz":-420},"id":"D04KoFBKCtxL","outputId":"4d14ad1e-b393-4447-861b-e1f6d4dc2efc"},"outputs":[{"name":"stdout","output_type":"stream","text":[">>> Installing ollama to /usr/local\n",">>> Downloading Linux amd64 bundle\n","######################################################################## 100.0%\n",">>> Creating ollama user...\n",">>> Adding ollama user to video group...\n",">>> Adding current user to ollama group...\n",">>> Creating ollama systemd service...\n","\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n","\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",">>> The Ollama API is now available at 127.0.0.1:11434.\n",">>> Install complete. Run \"ollama\" from the command line.\n","Collecting ollama\n","  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n","Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.12.3)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.12.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.11)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.41.4)\n","Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\n","Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.2)\n","Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n","Installing collected packages: ollama\n","Successfully installed ollama-0.6.1\n"]}],"source":["!curl -fsSL https://ollama.com/install.sh | sh\n","!pip install ollama tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2625,"status":"ok","timestamp":1767262500468,"user":{"displayName":"Minh Nguy·ªÖn","userId":"04212426870901819046"},"user_tz":-420},"id":"gzYzOmbvCwQu","outputId":"203113ec-8fac-4d10-b789-b62e218de5d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","pciutils is already the newest version (1:3.7.0-6).\n","0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"]}],"source":["!sudo apt-get install -y pciutils"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ylMf6H1eNcJH","outputId":"875d4015-accf-494b-82de-bba5b6669bbb"},"outputs":[{"name":"stdout","output_type":"stream","text":["üîÑ ƒêang kh·ªüi ƒë·ªông Ollama Server...\n"]}],"source":["import subprocess\n","import time\n","import os\n","\n","# Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng ƒë·ªÉ Ollama l·∫Øng nghe\n","os.environ = '0.0.0.0:11434'\n","os.environ = '*'\n","\n","print(\"üîÑ ƒêang kh·ªüi ƒë·ªông Ollama Server...\")\n","# Ch·∫°y server d∆∞·ªõi background\n","process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","\n","# ƒê·ª£i 10s ƒë·ªÉ server kh·ªüi ƒë·ªông\n","time.sleep(10)\n","\n","print(\"‚¨áÔ∏è ƒêang t·∫£i model gpt-oss:20b (Kho·∫£ng 14GB)...\")\n","# T·∫£i model (M·∫•t kho·∫£ng 5-10 ph√∫t t√πy m·∫°ng Colab)\n","!ollama pull gpt-oss:20b\n","\n","print(\"‚úÖ Model ƒë√£ s·∫µn s√†ng!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4496,"status":"ok","timestamp":1767264887534,"user":{"displayName":"Minh Nguy·ªÖn","userId":"04212426870901819046"},"user_tz":-420},"id":"bNzdNk3ECyHf","outputId":"b084120f-035a-4b9e-8ab6-9e3842409aca"},"outputs":[{"name":"stdout","output_type":"stream","text":["üöÄ B·∫Øt ƒë·∫ßu sinh d·ªØ li·ªáu v·ªõi gpt-oss:20b tr√™n GPU T4...\n"]},{"name":"stderr","output_type":"stream","text":["Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5270/5270 [00:04<00:00, 1216.75it/s]\n"]}],"source":["import json\n","import ollama\n","from tqdm import tqdm\n","import time\n","import os\n","\n","# --- C·∫§U H√åNH ---\n","MODEL_NAME = \"gpt-oss:20b\"\n","INPUT_FILE = \"clean_data.jsonl\" # File code b·∫°n ƒë√£ t·∫£i t·ª´ b∆∞·ªõc tr∆∞·ªõc\n","OUTPUT_FILE = \"gpt_oss_synthetic_data.jsonl\"\n","\n","# Prompt chuy√™n gia cho gpt-oss (T·∫≠n d·ª•ng kh·∫£ nƒÉng CoT c·ªßa model n√†y)\n","SYSTEM_PROMPT = \"\"\"You are an expert in Formal Verification and Promela/SPIN.\n","Analyze the user's Promela code and generate a high-quality instruction for it.\n","Your response must be a JSON object with this exact structure:\n","{\n","  \"summary\": \"Brief explanation of the logic\",\n","  \"instruction\": \"A clear, natural language request asking to write this code\",\n","  - \"processes\": A list of processes defined in the code.\n","- \"properties\": A list of LTL formulas or assertions checked.\n","}\n","Respond ONLY with the JSON object.\"\"\"\n","\n","def generate_with_local_gpt(code_content):\n","    try:\n","        response = ollama.chat(\n","            model=MODEL_NAME,\n","             messages=[\n","                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n","                {\"role\": \"user\", \"content\": code_content}\n","            ],\n","            format=\"json\", # √âp bu·ªôc tr·∫£ v·ªÅ JSON chu·∫©n\n","            options={\n","                \"temperature\": 0.2, # Gi·∫£m s√°ng t·∫°o ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c\n","                \"num_ctx\": 8192,    # Context window l·ªõn\n","                \"top_p\": 0.1\n","            }\n","        )\n","        return json.loads(response['message']['content'])\n","    except Exception as e:\n","        # print(f\"Error: {e}\") # B·∫≠t l√™n n·∫øu mu·ªën debug\n","        return None\n","\n","def main():\n","    if not os.path.exists(INPUT_FILE):\n","        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file {INPUT_FILE}. H√£y upload file v√†o Colab!\")\n","        return\n","\n","    print(f\"üöÄ B·∫Øt ƒë·∫ßu sinh d·ªØ li·ªáu v·ªõi {MODEL_NAME} tr√™n GPU T4...\")\n","\n","    # ƒê·∫øm d√≤ng\n","    total_lines = sum(1 for _ in open(INPUT_FILE, 'r', encoding='utf-8'))\n","\n","    with open(INPUT_FILE, 'r', encoding='utf-8') as f_in, \\\n","         open(OUTPUT_FILE, 'a', encoding='utf-8') as f_out:\n","\n","        for line in tqdm(f_in, total=total_lines, desc=\"Processing\"):\n","            if not line.strip(): continue\n","\n","            try:\n","                # 1. ƒê·ªçc d·ªØ li·ªáu th√¥\n","                record = json.loads(line)\n","                code = record.get('output') or record.get('content') or record.get('code')\n","\n","                if not code or len(code) < 50: continue\n","\n","                # 2. G·ªçi Local LLM (Kh√¥ng t·ªën ti·ªÅn, kh√¥ng rate limit)\n","                analysis = generate_with_local_gpt(code)\n","\n","                if analysis:\n","                    # 3. L∆∞u format chu·∫©n\n","                    train_sample = {\n","                        \"conversations\": [\n","                            {\"from\": \"human\", \"value\": analysis['instruction']},\n","                            {\"from\": \"gpt\", \"value\": code}\n","                        ],\n","                        \"metadata\": {\n","                            \"summary\": analysis.get('summary', ''),\n","                            \"properties\": analysis.get('properties',)\n","                        }\n","                    }\n","                    f_out.write(json.dumps(train_sample, ensure_ascii=False) + '\\n')\n","                    f_out.flush()\n","\n","            except json.JSONDecodeError:\n","                continue\n","            except Exception as e:\n","                continue\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":38,"status":"error","timestamp":1767264963490,"user":{"displayName":"Minh Nguy·ªÖn","userId":"04212426870901819046"},"user_tz":-420},"id":"EUrnFqVyMuKj","outputId":"3462c5a7-e9ea-40b1-8db3-0650f5b1b988"},"outputs":[{"ename":"ConnectionError","evalue":"Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1171896736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m resp = ollama.chat(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-oss:20b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     messages=[\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ollama/_client.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self, model, messages, tools, stream, think, logprobs, top_logprobs, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mChatResponse\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motherwise\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mChatResponse\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \"\"\"\n\u001b[0;32m--> 365\u001b[0;31m     return self._request(\n\u001b[0m\u001b[1;32m    366\u001b[0m       \u001b[0mChatResponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m       \u001b[0;34m'POST'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ollama/_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ollama/_client.py\u001b[0m in \u001b[0;36m_request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONNECTION_ERROR_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mConnectionError\u001b[0m: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download"]}],"source":["\n","import jsonimport json\n","import ollama\n","from tqdm import tqdm\n","import time\n","import os\n","resp = ollama.chat(\n","    model=\"gpt-oss:20b\",\n","    messages=[\n","        {\"role\": \"user\", \"content\": \"Say hello in one short sentence.\"}\n","    ]\n",")\n","\n","print(resp[\"message\"][\"content\"])"]},{"cell_type":"code","source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"folder_name\", # YOUR MODEL YOU USED FOR TRAINING\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n",")\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 64)"],"metadata":{"id":"NWxowUIa6Y-B"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["AfhobhIVmRNV"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}